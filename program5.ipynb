{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUcxfss2TMX0fv3XwNbJw1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4VblZWXigII","executionInfo":{"status":"ok","timestamp":1745148214932,"user_tz":-330,"elapsed":35931,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"3363e3fa-5135-488c-cfe8-87e85d2a932a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m522.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.14.1\n","    Uninstalling scipy-1.14.1:\n","      Successfully uninstalled scipy-1.14.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]}],"source":["%pip install gensim transformers nltk"]},{"cell_type":"code","source":["import gensim.downloader as api\n","import random\n","import nltk\n","from nltk.tokenize import word_tokenize\n","nltk.download('punkt')\n","\n","print(\"Loading pre-trained word vectors...\")\n","word_vectors = api.load('glove-wiki-gigaword-100')\n","print(\"Word vectors loaded.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxVjE9MZmcMS","executionInfo":{"status":"ok","timestamp":1745148669951,"user_tz":-330,"elapsed":71109,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"9f7c4d94-eb2c-480b-8377-a7ca13bafd45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Loading pre-trained word vectors...\n","[==================================================] 100.0% 128.1/128.1MB downloaded\n","Word vectors loaded.\n"]}]},{"cell_type":"code","source":["def get_similar_words(seed_word, top_n=5):\n","  try:\n","    similar_words = word_vectors.most_similar(seed_word, topn=top_n)\n","    return [word[0] for word in similar_words] #############<--------\n","  except KeyError:\n","    print(f\"Word '{seed_word}' not found in vocabulary.\")\n","    return []\n","\n","\n","def generate_sentence(seed_word, similar_words):\n","  sentence_templates = [f\"The {seed_word} was surrounded by {similar_words[0]} and {similar_words[1]}.\",\n","                        f\"People often associate {seed_word} with {similar_words[2]} and {similar_words[3]}.\",\n","                        f\"In land of {seed_word}, {similar_words[4]} was a common sight.\"\n","                        f\"A story about {seed_word} would be incomplete without {similar_words[1]} and {similar_words[3]}.\"]\n","  return random.choice(sentence_templates)\n","\n","\n","\n","def generate_paragraph(seed_word):\n","  similar_words = get_similar_words(seed_word)\n","  if not similar_words:\n","    return \"Could not generate a paragraph. Try another seed word.\"\n","  paragraph = [generate_sentence(seed_word, similar_words) for _ in range(4)]\n","  return \" \".join(paragraph)\n","\n","\n","seed_word = input(\"Enter a seed word: \")\n","paragraph = generate_paragraph(seed_word)\n","print(\"\\nGenerated Paragraph:\")\n","print(paragraph)"],"metadata":{"id":"5hG5TxMfijiF","executionInfo":{"status":"ok","timestamp":1745148704305,"user_tz":-330,"elapsed":3126,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a584c8c3-58bf-4bbe-c8d4-7810c8abca8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a seed word: silver\n","\n","Generated Paragraph:\n","In land of silver, medals was a common sight.A story about silver would be incomplete without bronze and medalist. In land of silver, medals was a common sight.A story about silver would be incomplete without bronze and medalist. People often associate silver with medal and medalist. People often associate silver with medal and medalist\n"]}]},{"cell_type":"markdown","source":["VIVA\n","\n","----\n","1. What is the significance of Cohere, LangChain, and Pydantic?\n","\n","- Cohere: Provides NLP APIs including embeddings, classification, and summarization via LLMs. Used for model inference.\n","\n","- LangChain: Framework to build applications powered by LLMs. It connects LLMs with tools like memory, APIs, and agents.\n","\n","- Pydantic: Used for data validation and parsing using Python type hints. Useful in defining schemas and ensuring structured output from LLMs.\n","\n","----\n","\n","2. Explain the techniques used in word embeddings.\n","\n","- Traditional: One-hot encoding, Bag of Words (BoW), TF-IDF.\n","\n","- Neural: Word2Vec (CBOW, Skip-gram), GloVe (co-occurrence matrix), FastText (subword n-grams), BERT (contextual embeddings).\n","\n","----\n","\n","3. Why do we use word embeddings?/ Need for word embeddings.\n","\n","- To reduce dimensionality and sparsity.\n","\n","- To capture semantic and syntactic meanings.\n","\n","- To enable words with similar meanings to have similar representations.\n","\n","----\n","\n","4. Discuss the real-world applications of LLMs and their limitations.\n","\n","- Applications: Chatbots, summarization, code generation, sentiment analysis, content creation.\n","\n","- Limitations: Bias in training data, hallucination, high computational cost, lack of real-time awareness.\n","\n","----\n","\n","5. Which model is used for summarization?\n","\n","- Pre-trained summarization model from Hugging Face (facebook/bart-large-cnn)\n","\n","----\n","\n","6. Explain the BART model in detail.\n","\n","- BART (Bidirectional and Auto-Regressive Transformer) combines BERT (encoding) and GPT (decoding).\n","\n","- It is a sequence-to-sequence model used for text generation, summarization, translation, etc.\n","\n","- Trained by corrupting text and learning to reconstruct it.\n","\n","----\n","\n","7. What is sentiment analysis and its applications?\n","\n","- It is the process of identifying sentiment (positive, negative, neutral) from text.\n","\n","- Applications: Customer feedback, brand monitoring, political analysis, market research.\n","\n","----\n","\n","8. Discuss and explain the significance of the parameter perplexity in t-SNE.\n","\n","- hyperparameter that defines the effective number of neighbors.\n","\n","- Controls the balance between local(less perplexity) vs. global(more perplexity) structure.\n","\n","- Should be less than the number of data points; typical range: 5–50.\n","\n","----\n","\n","9. Describe the algorithm (step-by-step, in words) for building an IPC chatbot.\n","\n","a. Download the Indian Penal Code document.\n","\n","b. Preprocess and split the document into retrievable chunks.\n","\n","c. Use embeddings to store the chunks in a vector store.\n","\n","d. Accept user queries.\n","\n","e. Retrieve relevant sections using similarity search.\n","\n","f. Use LLM (via LangChain) to answer based on the retrieved context.\n","\n","----\n","\n","10. Discuss PCA and t-SNE.\n","\n","- PCA: Linear, preserves global variance, faster, used for large datasets.\n","\n","- t-SNE: Non-linear, preserves local relationships, ideal for visualizing word clusters in small data.\n","\n","- Used to visualize high-dimensional word embeddings in 2D/3D.\n","\n","----\n","\n","11. What are the uses of prompt engineering?\n","\n","- To control LLM outputs by carefully designing the input prompts.\n","\n","- Used in chatbots, summarization, translation, data extraction, and few-shot learning.\n","\n","----"],"metadata":{"id":"WhyQ_sgqb7up"}}]}