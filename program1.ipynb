{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLZ4Fts4dMh8TUpWWyWI/0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"og-ANZfXAuQd","executionInfo":{"status":"ok","timestamp":1749067649314,"user_tz":-330,"elapsed":26285,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"548270a4-0862-4bc7-acc1-83e0e9986168"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m585.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.15.3\n","    Uninstalling scipy-1.15.3:\n","      Successfully uninstalled scipy-1.15.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"]}],"source":["#explore word relationships using vector arithmetic; perform arithmetic\n","%pip install numpy gensim"]},{"cell_type":"code","source":["import gensim.downloader as api\n","import numpy as np\n","from numpy.linalg import norm\n","\n","print(\"loading pre trained vectors\")\n","word_vectors = api.load(\"word2vec-google-news-300\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_gkbAvkBFyw","executionInfo":{"status":"ok","timestamp":1749068022030,"user_tz":-330,"elapsed":302095,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"325b0d78-a4db-45b0-dc97-f79bee407d30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["loading pre trained vectors\n","[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]}]},{"cell_type":"code","source":["#perform vector arithmetic and print similar words excluding i/p\n","def explore_word_relationships(word1, word2, word3):\n","  try:\n","    vec1 = word_vectors[word1]\n","    vec2 = word_vectors[word2]\n","    vec3 = word_vectors[word3]\n","\n","    #arithmetic\n","    result_vector = vec1 - vec2 + vec3\n","\n","    #most similar\n","    similar_words = word_vectors.similar_by_vector(result_vector, topn=10)\n","\n","    #exclude i/p\n","    input_words = {word1, word2, word3}\n","    filtered_words = [(word, similarity) for word, similarity in similar_words if word not in input_words]\n","\n","    print(f\"\\n word relationship: {word1} - {word2} + {word3}\")\n","    print(\"\\n most similar words excluding i/p\")\n","    for word, similarity in filtered_words[:5]:\n","      print(f\"{word} : {similarity:.4f}\")\n","\n","  except KeyError as e:\n","    print(f\"error: {e} not found in vocabulary\")\n","\n","explore_word_relationships(\"king\", \"man\", \"woman\")\n","explore_word_relationships(\"paris\", \"france\", \"germany\")\n","explore_word_relationships(\"apple\", \"fruit\", \"avocado\")\n","\n","#analyze similarity between 2 words\n","def analyze_similarity(word1, word2):\n","  try:\n","    similarity = word_vectors.similarity(word1, word2)\n","    print(f\"\\n similarity between '{word1}' and '{word2}' : {similarity:.4f}\")\n","  except KeyError as e:\n","    print(f\"Error: {e} not found in vocabulary\")\n","\n","analyze_similarity(\"cat\", \"dog\")\n","analyze_similarity(\"computer\", \"keyboard\")\n","analyze_similarity(\"music\", \"art\")\n","\n","#to find most similar words to given word\n","def find_most_similar(word):\n","  try:\n","    similar_words = word_vectors.most_similar(word, topn=5)\n","    print(f\"\\n most similar words to '{word}'\")\n","    for similar_word, similarity in similar_words:\n","      print(f\"{similar_word} : {similarity:.4f}\")\n","  except KeyError as e:\n","    print(f\"Error: {e} not found in vocabulary\")\n","\n","\n","find_most_similar(\"happy\")\n","find_most_similar(\"sad\")\n","find_most_similar(\"technology\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DatvtY4mD2eg","executionInfo":{"status":"ok","timestamp":1749073357896,"user_tz":-330,"elapsed":23873,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"69fc1b72-0e46-43d6-8bcb-02c81cb2e847"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," word relationship: king - man + woman\n","\n"," most similar words excluding i/p\n","queen : 0.7301\n","monarch : 0.6455\n","princess : 0.6156\n","crown_prince : 0.5819\n","prince : 0.5777\n","\n"," word relationship: paris - france + germany\n","\n"," most similar words excluding i/p\n","berlin : 0.4838\n","german : 0.4695\n","lindsay_lohan : 0.4536\n","switzerland : 0.4468\n","heidi : 0.4445\n","\n"," word relationship: apple - fruit + avocado\n","\n"," most similar words excluding i/p\n","almond : 0.5577\n","artichoke : 0.5569\n","onion : 0.5379\n","diced_avocado : 0.5322\n","chopped_almonds : 0.5236\n","\n"," similarity between 'cat' and 'dog' : 0.7609\n","\n"," similarity between 'computer' and 'keyboard' : 0.3964\n","\n"," similarity between 'music' and 'art' : 0.4010\n","\n"," most similar words to 'happy'\n","glad : 0.7409\n","pleased : 0.6632\n","ecstatic : 0.6627\n","overjoyed : 0.6599\n","thrilled : 0.6514\n","\n"," most similar words to 'sad'\n","saddening : 0.7273\n","Sad : 0.6611\n","saddened : 0.6604\n","heartbreaking : 0.6574\n","disheartening : 0.6507\n","\n"," most similar words to 'technology'\n","technologies : 0.8332\n","innovations : 0.6231\n","technological_innovations : 0.6102\n","technol : 0.6047\n","technological_advancement : 0.6036\n"]}]},{"cell_type":"markdown","source":["VIVA\n","\n","----\n","1. What is the significance of Cohere, LangChain, and Pydantic?\n","\n","- Cohere: Provides NLP APIs including embeddings, classification, and summarization via LLMs. Used for model inference.\n","\n","- LangChain: Framework to build applications powered by LLMs. It connects LLMs with tools like memory, APIs, and agents.\n","\n","- Pydantic: Used for data validation and parsing using Python type hints. Useful in defining schemas and ensuring structured output from LLMs.\n","\n","----\n","\n","2. Explain the techniques used in word embeddings.\n","\n","- Traditional: One-hot encoding, Bag of Words (BoW), TF-IDF.\n","\n","- Neural: Word2Vec (CBOW, Skip-gram), GloVe (co-occurrence matrix), FastText (subword n-grams), BERT (contextual embeddings).\n","\n","----\n","\n","3. Why do we use word embeddings?/ Need for word embeddings.\n","\n","- To reduce dimensionality and sparsity.\n","\n","- To capture semantic and syntactic meanings.\n","\n","- To enable words with similar meanings to have similar representations.\n","\n","----\n","\n","4. Discuss the real-world applications of LLMs and their limitations.\n","\n","- Applications: Chatbots, summarization, code generation, sentiment analysis, content creation.\n","\n","- Limitations: Bias in training data, hallucination, high computational cost, lack of real-time awareness.\n","\n","----\n","\n","5. Which model is used for summarization?\n","\n","- Pre-trained summarization model from Hugging Face (facebook/bart-large-cnn)\n","\n","----\n","\n","6. Explain the BART model in detail.\n","\n","- BART (Bidirectional and Auto-Regressive Transformer) combines BERT (encoding) and GPT (decoding).\n","\n","- It is a sequence-to-sequence model used for text generation, summarization, translation, etc.\n","\n","- Trained by corrupting text and learning to reconstruct it.\n","\n","----\n","\n","7. What is sentiment analysis and its applications?\n","\n","- It is the process of identifying sentiment (positive, negative, neutral) from text.\n","\n","- Applications: Customer feedback, brand monitoring, political analysis, market research.\n","\n","----\n","\n","8. Discuss and explain the significance of the parameter perplexity in t-SNE.\n","\n","- hyperparameter that defines the effective number of neighbors.\n","\n","- Controls the balance between local(less perplexity) vs. global(more perplexity) structure.\n","\n","- Should be less than the number of data points; typical range: 5–50.\n","\n","----\n","\n","9. Describe the algorithm (step-by-step, in words) for building an IPC chatbot.\n","\n","a. Download the Indian Penal Code document.\n","\n","b. Preprocess and split the document into retrievable chunks.\n","\n","c. Use embeddings to store the chunks in a vector store.\n","\n","d. Accept user queries.\n","\n","e. Retrieve relevant sections using similarity search.\n","\n","f. Use LLM (via LangChain) to answer based on the retrieved context.\n","\n","----\n","\n","10. Discuss PCA and t-SNE.\n","\n","- PCA: Linear, preserves global variance, faster, used for large datasets.\n","\n","- t-SNE: Non-linear, preserves local relationships, ideal for visualizing word clusters in small data.\n","\n","- Used to visualize high-dimensional word embeddings in 2D/3D.\n","\n","----\n","\n","11. What are the uses of prompt engineering?\n","\n","- To control LLM outputs by carefully designing the input prompts.\n","\n","- Used in chatbots, summarization, translation, data extraction, and few-shot learning.\n","\n","----"],"metadata":{"id":"WhyQ_sgqb7up"}}]}