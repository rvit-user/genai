{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfF/sbU3tLDqt3OzOvwC1C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24S9pNaEC_O4","executionInfo":{"status":"ok","timestamp":1746995549928,"user_tz":-330,"elapsed":14160,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"77072965-681a-4659-f5f0-7933bddfe773"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wikipedia-api\n","  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from wikipedia-api) (2.32.3)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.13.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (2025.4.26)\n","Building wheels for collected packages: wikipedia-api\n","  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=83eb371508dd07a53654ae7d70e9e30a3aad14bd5274e9350f73407ab0eaeae6\n","  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n","Successfully built wikipedia-api\n","Installing collected packages: wikipedia-api\n","Successfully installed wikipedia-api-0.8.1\n"]}],"source":["%pip install wikipedia-api pydantic"]},{"cell_type":"code","source":["from pydantic import BaseModel\n","from typing import List, Optional\n","import wikipediaapi\n","\n","class InstitutionDetails(BaseModel):\n","  founder: Optional[str]\n","  founded: Optional[str]\n","  branches: Optional[List[str]]\n","  number_of_employees: Optional[int]\n","  summary: Optional[str]\n","\n","def fetch_institution_details(name: str) -> InstitutionDetails:\n","  wiki_wiki = wikipediaapi.Wikipedia(user_agent=\"Script/1.0 (contact: myemail@example.com)\", language='en')\n","  page = wiki_wiki.page(name)\n","  #print(f\"Page: {page}\")\n","\n","  if not page.exists():\n","    raise ValueError(f\"No page found for '{name}'\")\n","\n","  founder = founded = None\n","  branches =[]\n","  summary = page.summary[:500]\n","  num_employees = None\n","\n","  for line in page.text.split('\\n'):\n","    if 'Founder' in line:\n","      founder = line.split(':')[-1].strip()\n","    elif 'Founded' in line:\n","      founded = line.split(':')[-1].strip()\n","    elif 'Branches' in line:\n","      branches = [b.strip() for b in line.split(':')[-1].split(',')]\n","    elif 'Number of employees' in line:\n","      try:\n","        num_employees = int(line.split(':')[-1].strip().replace(',', ''))\n","      except:\n","        pass\n","\n","  return InstitutionDetails(\n","      founder = founder,\n","      founded = founded,\n","      branches = branches or None,\n","      number_of_employees = num_employees,\n","      summary = summary\n","  )\n","\n","def display_details(d: InstitutionDetails):\n","  print(f\"\\n Institution Details:- \")\n","  print(f\"Founder: {d.founder or 'N/A'}\")\n","  print(f\"Founded: {d.founded or 'N/A'}\")\n","  print(f\"Branches: {', '.join(d.branches) if d.branches else 'N/A'}\")\n","  print(f\"Employees: {d.number_of_employees or 'N/A'}\")\n","  print(f\"Summary: {d.summary or 'N/A'}\")\n","\n","name = input(\"Enter institution name: \").strip()\n","if name:\n","  try:\n","    details = fetch_institution_details(name)\n","    display_details(details)\n","  except Exception as e:\n","    print(\"Error : \", e)\n","else:\n","  print(\"Please enter a valid name!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cGI3lANwDD_U","executionInfo":{"status":"ok","timestamp":1746996353505,"user_tz":-330,"elapsed":14072,"user":{"displayName":"Anshul B A","userId":"04424757554927801991"}},"outputId":"f0df4318-1ede-48f1-8353-fafb2783a833"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter institution name: Massachusetts Institute of Technology\n","\n"," Institution Details:- \n","Founder: N/A\n","Founded: N/A\n","Branches: N/A\n","Employees: N/A\n","Summary: The Massachusetts Institute of Technology (MIT) is a private research university in Cambridge, Massachusetts, United States. Established in 1861, MIT has played a significant role in the development of many areas of modern technology and science.\n","In response to the increasing industrialization of the United States, William Barton Rogers organized a school in Boston to create \"useful knowledge.\" Initially funded by a federal land grant, the institute adopted a polytechnic model that stressed labo\n"]}]},{"cell_type":"markdown","source":["VIVA\n","\n","----\n","1. What is the significance of Cohere, LangChain, and Pydantic?\n","\n","- Cohere: Provides NLP APIs including embeddings, classification, and summarization via LLMs. Used for model inference.\n","\n","- LangChain: Framework to build applications powered by LLMs. It connects LLMs with tools like memory, APIs, and agents.\n","\n","- Pydantic: Used for data validation and parsing using Python type hints. Useful in defining schemas and ensuring structured output from LLMs.\n","\n","----\n","\n","2. Explain the techniques used in word embeddings.\n","\n","- Traditional: One-hot encoding, Bag of Words (BoW), TF-IDF.\n","\n","- Neural: Word2Vec (CBOW, Skip-gram), GloVe (co-occurrence matrix), FastText (subword n-grams), BERT (contextual embeddings).\n","\n","----\n","\n","3. Why do we use word embeddings?/ Need for word embeddings.\n","\n","- To reduce dimensionality and sparsity.\n","\n","- To capture semantic and syntactic meanings.\n","\n","- To enable words with similar meanings to have similar representations.\n","\n","----\n","\n","4. Discuss the real-world applications of LLMs and their limitations.\n","\n","- Applications: Chatbots, summarization, code generation, sentiment analysis, content creation.\n","\n","- Limitations: Bias in training data, hallucination, high computational cost, lack of real-time awareness.\n","\n","----\n","\n","5. Which model is used for summarization?\n","\n","- Pre-trained summarization model from Hugging Face (facebook/bart-large-cnn)\n","\n","----\n","\n","6. Explain the BART model in detail.\n","\n","- BART (Bidirectional and Auto-Regressive Transformer) combines BERT (encoding) and GPT (decoding).\n","\n","- It is a sequence-to-sequence model used for text generation, summarization, translation, etc.\n","\n","- Trained by corrupting text and learning to reconstruct it.\n","\n","----\n","\n","7. What is sentiment analysis and its applications?\n","\n","- It is the process of identifying sentiment (positive, negative, neutral) from text.\n","\n","- Applications: Customer feedback, brand monitoring, political analysis, market research.\n","\n","----\n","\n","8. Discuss and explain the significance of the parameter perplexity in t-SNE.\n","\n","- hyperparameter that defines the effective number of neighbors.\n","\n","- Controls the balance between local(less perplexity) vs. global(more perplexity) structure.\n","\n","- Should be less than the number of data points; typical range: 5â€“50.\n","\n","----\n","\n","9. Describe the algorithm (step-by-step, in words) for building an IPC chatbot.\n","\n","a. Download the Indian Penal Code document.\n","\n","b. Preprocess and split the document into retrievable chunks.\n","\n","c. Use embeddings to store the chunks in a vector store.\n","\n","d. Accept user queries.\n","\n","e. Retrieve relevant sections using similarity search.\n","\n","f. Use LLM (via LangChain) to answer based on the retrieved context.\n","\n","----\n","\n","10. Discuss PCA and t-SNE.\n","\n","- PCA: Linear, preserves global variance, faster, used for large datasets.\n","\n","- t-SNE: Non-linear, preserves local relationships, ideal for visualizing word clusters in small data.\n","\n","- Used to visualize high-dimensional word embeddings in 2D/3D.\n","\n","----\n","\n","11. What are the uses of prompt engineering?\n","\n","- To control LLM outputs by carefully designing the input prompts.\n","\n","- Used in chatbots, summarization, translation, data extraction, and few-shot learning.\n","\n","----"],"metadata":{"id":"WhyQ_sgqb7up"}}]}